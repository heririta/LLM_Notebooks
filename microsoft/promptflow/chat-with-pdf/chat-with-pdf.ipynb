{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with PDF - test, evaluation and experimentation\n",
    "\n",
    "# Develop a flow\n",
    "\n",
    "https://microsoft.github.io/promptflow/how-to-guides/develop-a-dag-flow/quick-start.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## env variables\n",
    "```\n",
    "AZURE_SUBSCRIPTION_ID= SUBCRIPTION_ID\n",
    "AZURE_RESOURCE_GROUP= RESOURCE GROUP\n",
    "AZUREAI_PROJECT_NAME= AZURE AI STUDIO PROJECT NAME\n",
    "AZURE_OPENAI_CONNECTION_NAME=  Azure OPENai Connection NAME\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT= AZURE OPENAI ENDPOINT URL\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT= AZURE OPENAI CHAT DEPLOYMENT NAME\n",
    "AZURE_OPENAI_API_VERSION= AZURE OPENAI CHAT DEPLOYMENT VERSION \n",
    "AZURE_OPENAI_API_KEY= AZURE OPENAI KEY\n",
    "\n",
    "RESOURCE_GROUP= RESOURCE GROUP NAME = AZURE_RESOURCE_GROUP\n",
    "SUBSCRIPTION_ID=  AZURE_SUBSCRIPTION_ID\n",
    "AZUREML_WORKSPACE_NAME= AZURE ML NAME \n",
    "TENANTID= TENANT ID SERVICE PRINCIPaL ACCOUNT\n",
    "AZURE_CLIENT_ID=   CLIENT  ID SERVICE PRINCIPaL ACCOUNT\n",
    "AZURE_TENANT_ID=  TENANT ID SERVICE PRINCIPaL ACCOUNT = TENANTID\n",
    "AZURE_CLIENT_SECRET=  SERVICE PRINCIPAL ACCOUNT SECRET\n",
    "\n",
    "OPENAI_API_TYPE=azure\n",
    "OPENAI_API_BASE=https://open-ai-olonok.openai.azure.com/\n",
    "OPENAI_API_KEY=\"\"\n",
    "OPENAI_API_VERSION=2024-02-15-preview\n",
    "EMBEDDING_MODEL_DEPLOYMENT_NAME=text-embedding-ada-002\n",
    "CHAT_MODEL_DEPLOYMENT_NAME=gpt-4\n",
    "PROMPT_TOKEN_LIMIT=3000\n",
    "MAX_COMPLETION_TOKENS=1024\n",
    "CHUNK_SIZE=256\n",
    "CHUNK_OVERLAP=64\n",
    "VERBOSE=False\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Requirements\n",
    "\n",
    "```\n",
    "\n",
    "python-dotenv\n",
    "bs4\n",
    "azure-identity\n",
    "azure-search-documents==11.4.0\n",
    "promptflow-tracing==1.11.0\n",
    "promptflow-evals==0.3.0\n",
    "jinja2\n",
    "aiohttp\n",
    "azure-ai-ml==1.16.0\n",
    "promptflow[azure]==1.11.0\n",
    "promptflow-tools==1.4.0\n",
    "promptflow-rag==0.1.0\n",
    "jinja2\n",
    "aiohttp\n",
    "\n",
    "\n",
    "# The following dependencies are required for provisioning\n",
    "\n",
    "# openai SDK\n",
    "openai==1.13.3\n",
    "\n",
    "# azure dependencies\n",
    "azure-core==1.30.1\n",
    "azure-mgmt-authorization==4.0.0\n",
    "azure-mgmt-resource==23.0.1\n",
    "azure-mgmt-search==9.1.0\n",
    "azure-mgmt-cognitiveservices==13.5.0\n",
    "\n",
    "# utilities\n",
    "omegaconf-argparse==1.0.1\n",
    "omegaconf==2.3.0\n",
    "pydantic>=2.6\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create connections\n",
    "Connection in prompt flow is for managing settings of your application behaviors incl. how to talk to different services (Azure OpenAI for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_ai_connection (AzureOpenAI)\n"
     ]
    }
   ],
   "source": [
    "import promptflow\n",
    "from promptflow.client import PFClient\n",
    "pf = PFClient()\n",
    "\n",
    "# List all the available connections\n",
    "for c in pf.connections.list():\n",
    "    print(c.name + \" (\" + c.type + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to have a connection named \"open_ai_connection\" to run the chat_with_pdf flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using existing connection\n",
      "auth_mode: key\n",
      "name: open_ai_connection\n",
      "module: promptflow.connections\n",
      "created_date: '2024-11-16T10:59:01.985404'\n",
      "last_modified_date: '2024-11-16T10:59:01.985404'\n",
      "type: azure_open_ai\n",
      "api_key: '******'\n",
      "api_base: https://open-ai-olonok.openai.azure.com/\n",
      "api_type: azure\n",
      "api_version: 2024-02-15-preview\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create needed connection\n",
    "from promptflow.entities import AzureOpenAIConnection, OpenAIConnection\n",
    "\n",
    "try:\n",
    "    conn_name = \"open_ai_connection\"\n",
    "    conn = pf.connections.get(name=conn_name)\n",
    "    print(\"using existing connection\")\n",
    "except:\n",
    "    # Follow https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource?pivots=web-portal to create an Azure OpenAI resource.\n",
    "    connection = AzureOpenAIConnection(\n",
    "        name=\"open_ai_connection\",\n",
    "        api_key=config.get(\"OPENAI_API_KEY\"),\n",
    "        api_base=config.get(\"OPENAI_API_BASE\"),\n",
    "        api_type=\"azure\",\n",
    "        api_version=config.get(\"OPENAI_API_VERSION\"),\n",
    "    )\n",
    "\n",
    "    # use this if you have an existing OpenAI account\n",
    "    # connection = OpenAIConnection(\n",
    "    #     name=conn_name,\n",
    "    #     api_key=\"<user-input>\",\n",
    "    # )\n",
    "    conn = pf.connections.create_or_update(connection)\n",
    "    print(\"successfully created connection\")\n",
    "\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test the flow\n",
    "\n",
    "**Note**: this sample uses `predownloaded PDFs` and `prebuilt FAISS Index` to speed up execution time.\n",
    "You can remove the folders to start a fresh run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Start to run 6 nodes with concurrency level 16.\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Executing node setup_env. node run id: bdaa42f8-0244-4011-8baa-36b328e92891_setup_env_0\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Node setup_env completes.\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Executing node download_tool. node run id: bdaa42f8-0244-4011-8baa-36b328e92891_download_tool_0\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Executing node rewrite_question_tool. node run id: bdaa42f8-0244-4011-8baa-36b328e92891_rewrite_question_tool_0\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     [download_tool in line 0 (index starts from 0)] stdout> Pdf already exists in D:\\repos2\\rag-data-openai-python-promptflow\\notebooks\\chat-with-pdf\\chat_with_pdf\\.pdfs\\https___arxiv.org_pdf_1810.04805.pdf.pdf\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Node download_tool completes.\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Executing node build_index_tool. node run id: bdaa42f8-0244-4011-8baa-36b328e92891_build_index_tool_0\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     [build_index_tool in line 0 (index starts from 0)] stdout> Chunk size: 1024, chunk overlap: 64\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     [build_index_tool in line 0 (index starts from 0)] stdout> Index path: D:\\repos2\\rag-data-openai-python-promptflow\\notebooks\\chat-with-pdf\\chat_with_pdf\\.index\\.pdfs\\https___arxiv.org_pdf_1810.04805.pdf.pdf.index_1024_64\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     [build_index_tool in line 0 (index starts from 0)] stdout> Index already exists, bypassing index creation\n",
      "2024-11-16 19:24:55 +0000   34368 execution.flow     INFO     Node build_index_tool completes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:24:56 +0000   34368 execution.flow     INFO     [rewrite_question_tool in line 0 (index starts from 0)] stdout> Rewritten question: What are unsupervised feature-based approaches?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 19:24:56 +0000   34368 execution.flow     WARNING  Failed to calculate metrics due to exception: unsupported operand type(s) for +: 'int' and 'NoneType'.\n",
      "2024-11-16 19:24:56 +0000   34368 execution.flow     INFO     Node rewrite_question_tool completes.\n",
      "2024-11-16 19:24:56 +0000   34368 execution.flow     INFO     Executing node find_context_tool. node run id: bdaa42f8-0244-4011-8baa-36b328e92891_find_context_tool_0\n",
      "2024-11-16 19:24:57 +0000   34368 execution.flow     INFO     Node find_context_tool completes.\n",
      "2024-11-16 19:24:57 +0000   34368 execution.flow     INFO     Executing node qna_tool. node run id: bdaa42f8-0244-4011-8baa-36b328e92891_qna_tool_0\n",
      "2024-11-16 19:25:01 +0000   34368 execution.flow     INFO     Node qna_tool completes.\n"
     ]
    }
   ],
   "source": [
    "# ./chat_with_pdf/.pdfs/ stores predownloaded PDFs\n",
    "# ./chat_with_pdf/.index/ stores prebuilt index files\n",
    "\n",
    "output = pf.flows.test(\n",
    "    \"./flow.dag.yaml\",\n",
    "    inputs={\n",
    "        \"chat_history\": [],\n",
    "        \"pdf_url\": \"https://arxiv.org/pdf/1810.04805.pdf\",\n",
    "        \"question\": \"what are  Unsupervised Feature-based Approaches?\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"Les approches basées sur les caractéristiques non supervisées impliquent l'utilisation de paramètres de plongement de mots pré-entraînés à partir de textes non étiquetés. Ces représentations sont ensuite ajustées pour une tâche en aval spécifique. Les avantages de ces approches incluent la nécessité de paramètres moins nombreux à apprendre à partir de zéro.\", 'context': ['ngio. 2010.\\nWord representations: A simple and general method\\nfor semi-supervised learning. In Proceedings of the\\n48th Annual Meeting of the Association for Compu-\\ntational Linguistics , ACL ’10, pages 384–394.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in Neural Information Pro-\\ncessing Systems , pages 6000–6010.\\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\\nPierre-Antoine Manzagol. 2008. Extracting and\\ncomposing robust features with denoising autoen-\\ncoders. In Proceedings of the 25th international\\nconference on Machine learning , pages 1096–1103.\\nACM.\\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\\nGlue: A multi-task benchmark and analysis platformfor natural language understanding. In Proceedings\\nof the 2018 EMNLP Workshop BlackboxNLP: An-\\nalyzing and Interpreting Neural Networks for NLP ,\\npages 353–355.\\nWei Wang, Ming Yan, and ', 'f text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approaches\\nAs with the feature-based approaches, the ﬁrst\\nworks in this direction only pre-trained word em-\\nbedding parameters from unlabeled text (Col-\\nlobert and Weston, 2008).\\nMore recently, sentence or document encoders\\nwhich produce contextual token representations\\nhave been pre-trained from unlabeled text and\\nﬁne-tuned for a supervised downstream task (Dai\\nand Le, 2015; Howard and Ruder, 2018; Radford\\net al., 2018). The advantage of these approaches\\nis that few parameters need to be learned from\\nscratch. At least partly due to this advantage,\\nOpenAI GPT (Radford et al., 2018) achieved pre-\\nviously state-of-the-art results on many sentence-\\nlevel tasks from the GLUE benchmark (Wang\\net al., 2018a). Left-to-right language model-BERT BERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’\\n[CLS] Tok 1 [SEP] ... Tok NTok 1 ... TokM \\nQuestion Paragraph Start/End Span \\nBERT \\nE[CLS] E1 E[SEP] ... ENE1’... EM’\\nC\\nT1\\nT[SEP] ...\\n TN\\nT1’...\\n TM’', ') generalize traditional word embedding re-\\nsearch along a different dimension. They extract\\ncontext-sensitive features from a left-to-right and a\\nright-to-left language model. The contextual rep-\\nresentation of each token is the concatenation of\\nthe left-to-right and right-to-left representations.\\nWhen integrating contextual word embeddings\\nwith existing task-speciﬁc architectures, ELMo\\nadvances the state of the art for several major NLP\\nbenchmarks (Peters et al., 2018a) including ques-\\ntion answering (Rajpurkar et al., 2016), sentiment\\nanalysis (Socher et al., 2013), and named entity\\nrecognition (Tjong Kim Sang and De Meulder,\\n2003). Melamud et al. (2016) proposed learning\\ncontextual representations through a task to pre-\\ndict a single word from both left and right context\\nusing LSTMs. Similar to ELMo, their model is\\nfeature-based and not deeply bidirectional. Fedus\\net al. (2018) shows that the cloze task can be used\\nto improve the robustness of text generation mod-\\nels.\\n2.2 Unsupervised Fine-tuning Approac', 'tioned in passing that increasing hidden dimen-\\nsion size from 200 to 600 helped, but increasing\\nfurther to 1,000 did not bring further improve-\\nments. Both of these prior works used a feature-\\nbased approach — we hypothesize that when the\\nmodel is ﬁne-tuned directly on the downstream\\ntasks and uses only a very small number of ran-\\ndomly initialized additional parameters, the task-\\nspeciﬁc models can beneﬁt from the larger, more\\nexpressive pre-trained representations even when\\ndownstream task data is very small.\\n5.3 Feature-based Approach with BERT\\nAll of the BERT results presented so far have used\\nthe ﬁne-tuning approach, where a simple classiﬁ-\\ncation layer is added to the pre-trained model, and\\nall parameters are jointly ﬁne-tuned on a down-\\nstream task. However, the feature-based approach,\\nwhere ﬁxed features are extracted from the pre-\\ntrained model, has certain advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore require\\na task-speciﬁc model a', ', pages 1532–\\n1543.\\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\\nula, and Russell Power. 2017. Semi-supervised se-\\nquence tagging with bidirectional language models.\\nInACL.\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018a. Deep contextualized word rep-\\nresentations. In NAACL .Matthew Peters, Mark Neumann, Luke Zettlemoyer,\\nand Wen-tau Yih. 2018b. Dissecting contextual\\nword embeddings: Architecture and representation.\\nInProceedings of the 2018 Conference on Empiri-\\ncal Methods in Natural Language Processing , pages\\n1499–1509.\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\\nIlya Sutskever. 2018. Improving language under-\\nstanding with unsupervised learning. Technical re-\\nport, OpenAI.\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100,000+ questions for\\nmachine comprehension of text. In Proceedings of\\nthe 2016 Conference on Empirical Methods in Nat-\\nural Language Processing , pages 2383–2392.\\nMinjoon Seo, ']}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the flow with a data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-16 19:30:02 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run chat_with_pdf_variant_0_20241116_193001_832639, log path: C:\\Users\\User\\.promptflow\\.runs\\chat_with_pdf_variant_0_20241116_193001_832639\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=chat_with_pdf_variant_0_20241116_193001_832639\n",
      "2024-11-16 19:30:02 +0000   34368 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-11-16 19:30:02 +0000   34368 execution.bulk     INFO     Current system's available memory is 37806.98828125MB, memory consumption of current process is 307.45703125MB, estimated available worker count is 37806.98828125/307.45703125 = 122\n",
      "2024-11-16 19:30:02 +0000   34368 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 122}.\n",
      "2024-11-16 19:30:10 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-24)-Process id(36792)-Line number(0) start execution.\n",
      "2024-11-16 19:30:10 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-25)-Process id(16868)-Line number(1) start execution.\n",
      "2024-11-16 19:30:10 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-26)-Process id(34340)-Line number(2) start execution.\n",
      "2024-11-16 19:30:18 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-24)-Process id(36792)-Line number(0) completed.\n",
      "2024-11-16 19:30:19 +0000   34368 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-11-16 19:30:19 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 9.08 seconds. Estimated time for incomplete lines: 18.16 seconds.\n",
      "2024-11-16 19:30:23 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-25)-Process id(16868)-Line number(1) completed.\n",
      "2024-11-16 19:30:24 +0000   34368 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2024-11-16 19:30:24 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 7.08 seconds. Estimated time for incomplete lines: 7.08 seconds.\n",
      "2024-11-16 19:30:28 +0000   34368 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-11-16 19:30:28 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 6.07 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-11-16 19:30:28 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-26)-Process id(34340)-Line number(2) completed.\n",
      "2024-11-16 19:30:28 +0000   34368 execution.bulk     INFO     The thread monitoring the process [36792-SpawnProcess-24] will be terminated.\n",
      "2024-11-16 19:30:28 +0000   34368 execution.bulk     INFO     The thread monitoring the process [16868-SpawnProcess-25] will be terminated.\n",
      "2024-11-16 19:30:28 +0000   34368 execution.bulk     INFO     The thread monitoring the process [34340-SpawnProcess-26] will be terminated.\n",
      "2024-11-16 19:30:28 +0000   36792 execution.bulk     INFO     The process [36792] has received a terminate signal.\n",
      "2024-11-16 19:30:28 +0000   16868 execution.bulk     INFO     The process [16868] has received a terminate signal.\n",
      "2024-11-16 19:30:28 +0000   34340 execution.bulk     INFO     The process [34340] has received a terminate signal.\n",
      "2024-11-16 19:30:29 +0000   34368 execution.bulk     INFO     Process 16868 terminated.\n",
      "2024-11-16 19:30:29 +0000   34368 execution.bulk     INFO     Process 36792 terminated.\n",
      "2024-11-16 19:30:29 +0000   34368 execution.bulk     INFO     Process 34340 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"chat_with_pdf_variant_0_20241116_193001_832639\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-11-16 19:30:01.797635+00:00\"\n",
      "Duration: \"0:00:29.366736\"\n",
      "Output path: \"C:\\Users\\User\\.promptflow\\.runs\\chat_with_pdf_variant_0_20241116_193001_832639\"\n",
      "\n",
      "name: chat_with_pdf_variant_0_20241116_193001_832639\n",
      "created_on: '2024-11-16T19:30:01.797635+00:00'\n",
      "status: Completed\n",
      "display_name: chat_with_pdf_variant_0_20241116_193001_832639\n",
      "description:\n",
      "tags:\n",
      "properties:\n",
      "  flow_path: D:/repos2/rag-data-openai-python-promptflow/notebooks/chat-with-pdf\n",
      "  output_path: C:/Users/User/.promptflow/.runs/chat_with_pdf_variant_0_20241116_193001_832639\n",
      "  column_mapping:\n",
      "    question: ${data.question}\n",
      "    pdf_url: ${data.pdf_url}\n",
      "    chat_history: ${data.chat_history}\n",
      "    config:\n",
      "      EMBEDDING_MODEL_DEPLOYMENT_NAME: text-embedding-ada-002\n",
      "      CHAT_MODEL_DEPLOYMENT_NAME: gpt-4\n",
      "      PROMPT_TOKEN_LIMIT: 2000\n",
      "      MAX_COMPLETION_TOKENS: 256\n",
      "      VERBOSE: true\n",
      "      CHUNK_SIZE: 1024\n",
      "      CHUNK_OVERLAP: 64\n",
      "  system_metrics:\n",
      "    total_tokens: 5244\n",
      "    prompt_tokens: 4962\n",
      "    completion_tokens: 282\n",
      "    duration: 28.495137\n",
      "flow_name: chat-with-pdf\n",
      "data: \n",
      "  D:/repos2/rag-data-openai-python-promptflow/notebooks/chat-with-pdf/data/bert-paper-qna-3-line.jsonl\n",
      "output: \n",
      "  C:/Users/User/.promptflow/.runs/chat_with_pdf_variant_0_20241116_193001_832639/flow_outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flow_path = \".\"\n",
    "data_path = \"./data/bert-paper-qna-3-line.jsonl\"\n",
    "\n",
    "config_2k_context = {\n",
    "    \"EMBEDDING_MODEL_DEPLOYMENT_NAME\": \"text-embedding-ada-002\",\n",
    "    \"CHAT_MODEL_DEPLOYMENT_NAME\": \"gpt-4\",  # change this to the name of your deployment if you're using Azure OpenAI\n",
    "    \"PROMPT_TOKEN_LIMIT\": 2000,\n",
    "    \"MAX_COMPLETION_TOKENS\": 256,\n",
    "    \"VERBOSE\": True,\n",
    "    \"CHUNK_SIZE\": 1024,\n",
    "    \"CHUNK_OVERLAP\": 64,\n",
    "}\n",
    "\n",
    "column_mapping = {\n",
    "    \"question\": \"${data.question}\",\n",
    "    \"pdf_url\": \"${data.pdf_url}\",\n",
    "    \"chat_history\": \"${data.chat_history}\",\n",
    "    \"config\": config_2k_context,\n",
    "}\n",
    "run_2k_context = pf.run(flow=flow_path, data=data_path, column_mapping=column_mapping)\n",
    "pf.stream(run_2k_context)\n",
    "\n",
    "print(run_2k_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.pdf_url</th>\n",
       "      <th>inputs.chat_history</th>\n",
       "      <th>inputs.config</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main difference between BERT and p...</td>\n",
       "      <td>https://arxiv.org/pdf/1810.04805.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'EMBEDDING_MODEL_DEPLOYMENT_NAME': 'text-embe...</td>\n",
       "      <td>0</td>\n",
       "      <td>The main difference between BERT and earlier l...</td>\n",
       "      <td>[BERT: Pre-training of Deep Bidirectional Tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the size of the vocabulary used by BERT?</td>\n",
       "      <td>https://arxiv.org/pdf/1810.04805.pdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'EMBEDDING_MODEL_DEPLOYMENT_NAME': 'text-embe...</td>\n",
       "      <td>1</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>[E (L=12, H=768, A=12, Total Param-\\neters=110...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>论文写作中论文引言有什么注意事项？</td>\n",
       "      <td>https://grs.pku.edu.cn/docs/2018-03/2018030108...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'EMBEDDING_MODEL_DEPLOYMENT_NAME': 'text-embe...</td>\n",
       "      <td>2</td>\n",
       "      <td>在论文写作中，论文引言的要点应该包括：简洁地提出研究问题或研究主题，阐明研究的重要性和必要性...</td>\n",
       "      <td>[须言之成理，论据可靠 ，严格遵循本学科国际通行的学术规范。 内容包括：第 一\\n章引言（或...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     inputs.question  \\\n",
       "0  What is the main difference between BERT and p...   \n",
       "1   What is the size of the vocabulary used by BERT?   \n",
       "2                                  论文写作中论文引言有什么注意事项？   \n",
       "\n",
       "                                      inputs.pdf_url inputs.chat_history  \\\n",
       "0               https://arxiv.org/pdf/1810.04805.pdf                  []   \n",
       "1               https://arxiv.org/pdf/1810.04805.pdf                  []   \n",
       "2  https://grs.pku.edu.cn/docs/2018-03/2018030108...                  []   \n",
       "\n",
       "                                       inputs.config  inputs.line_number  \\\n",
       "0  {'EMBEDDING_MODEL_DEPLOYMENT_NAME': 'text-embe...                   0   \n",
       "1  {'EMBEDDING_MODEL_DEPLOYMENT_NAME': 'text-embe...                   1   \n",
       "2  {'EMBEDDING_MODEL_DEPLOYMENT_NAME': 'text-embe...                   2   \n",
       "\n",
       "                                      outputs.answer  \\\n",
       "0  The main difference between BERT and earlier l...   \n",
       "1                                      I don't know.   \n",
       "2  在论文写作中，论文引言的要点应该包括：简洁地提出研究问题或研究主题，阐明研究的重要性和必要性...   \n",
       "\n",
       "                                     outputs.context  \n",
       "0  [BERT: Pre-training of Deep Bidirectional Tran...  \n",
       "1  [E (L=12, H=768, A=12, Total Param-\\neters=110...  \n",
       "2  [须言之成理，论据可靠 ，严格遵循本学科国际通行的学术规范。 内容包括：第 一\\n章引言（或...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_details(run_2k_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the \"groundedness\"\n",
    "The `eval-groundedness flow` is using ChatGPT/GPT4 model to grade the answers generated by chat-with-pdf flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=eval_groundedness_variant_0_20241116_120943_262113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-16 12:09:43 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run eval_groundedness_variant_0_20241116_120943_262113, log path: C:\\Users\\User\\.promptflow\\.runs\\eval_groundedness_variant_0_20241116_120943_262113\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-16 12:09:44 +0000   34368 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-11-16 12:09:44 +0000   34368 execution.bulk     INFO     Current system's available memory is 39408.26171875MB, memory consumption of current process is 290.78125MB, estimated available worker count is 39408.26171875/290.78125 = 135\n",
      "2024-11-16 12:09:44 +0000   34368 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 135}.\n",
      "2024-11-16 12:09:52 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-6)-Process id(35100)-Line number(0) start execution.\n",
      "2024-11-16 12:09:52 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-8)-Process id(17748)-Line number(1) start execution.\n",
      "2024-11-16 12:09:52 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(4352)-Line number(2) start execution.\n",
      "2024-11-16 12:09:53 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-6)-Process id(35100)-Line number(0) completed.\n",
      "2024-11-16 12:09:54 +0000   34368 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-11-16 12:09:54 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 2.01 seconds. Estimated time for incomplete lines: 4.02 seconds.\n",
      "2024-11-16 12:10:00 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(4352)-Line number(2) completed.\n",
      "2024-11-16 12:10:00 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-8)-Process id(17748)-Line number(1) completed.\n",
      "2024-11-16 12:10:01 +0000   34368 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-11-16 12:10:01 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 3.03 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-11-16 12:10:01 +0000   34368 execution.bulk     INFO     The thread monitoring the process [4352-SpawnProcess-7] will be terminated.\n",
      "2024-11-16 12:10:01 +0000   34368 execution.bulk     INFO     The thread monitoring the process [17748-SpawnProcess-8] will be terminated.\n",
      "2024-11-16 12:10:01 +0000   34368 execution.bulk     INFO     The thread monitoring the process [35100-SpawnProcess-6] will be terminated.\n",
      "2024-11-16 12:10:01 +0000   17748 execution.bulk     INFO     The process [17748] has received a terminate signal.\n",
      "2024-11-16 12:10:01 +0000   35100 execution.bulk     INFO     The process [35100] has received a terminate signal.\n",
      "2024-11-16 12:10:02 +0000   34368 execution.bulk     INFO     Process 4352 terminated.\n",
      "2024-11-16 12:10:02 +0000   34368 execution.bulk     INFO     Process 35100 terminated.\n",
      "2024-11-16 12:10:02 +0000   34368 execution.bulk     INFO     Process 17748 terminated.\n",
      "2024-11-16 12:10:03 +0000   34368 execution.bulk     INFO     Executing aggregation node...\n",
      "2024-11-16 12:10:03 +0000   34368 execution.bulk     INFO     Finish executing aggregation node.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"eval_groundedness_variant_0_20241116_120943_262113\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-11-16 12:09:43.205785+00:00\"\n",
      "Duration: \"0:00:20.433963\"\n",
      "Output path: \"C:\\Users\\User\\.promptflow\\.runs\\eval_groundedness_variant_0_20241116_120943_262113\"\n",
      "\n",
      "name: eval_groundedness_variant_0_20241116_120943_262113\n",
      "created_on: '2024-11-16T12:09:43.205785+00:00'\n",
      "status: Completed\n",
      "display_name: eval_groundedness_2k_context\n",
      "description:\n",
      "tags:\n",
      "properties:\n",
      "  flow_path: \n",
      "    D:/repos2/rag-data-openai-python-promptflow/notebooks/evaluation/eval-groundedness\n",
      "  output_path: \n",
      "    C:/Users/User/.promptflow/.runs/eval_groundedness_variant_0_20241116_120943_262113\n",
      "  column_mapping:\n",
      "    question: ${run.inputs.question}\n",
      "    answer: ${run.outputs.answer}\n",
      "    context: ${run.outputs.context}\n",
      "  system_metrics:\n",
      "    total_tokens: 4740\n",
      "    prompt_tokens: 4737\n",
      "    completion_tokens: 3\n",
      "    duration: 19.538268\n",
      "flow_name: eval-groundedness\n",
      "data:\n",
      "output: \n",
      "  C:/Users/User/.promptflow/.runs/eval_groundedness_variant_0_20241116_120943_262113/flow_outputs\n",
      "run: chat_with_pdf_variant_0_20241116_120643_052431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_groundedness_flow_path = \"../evaluation/eval-groundedness/\"\n",
    "eval_groundedness_2k_context = pf.run(\n",
    "    flow=eval_groundedness_flow_path,\n",
    "    run=run_2k_context,\n",
    "    column_mapping={\n",
    "        \"question\": \"${run.inputs.question}\",\n",
    "        \"answer\": \"${run.outputs.answer}\",\n",
    "        \"context\": \"${run.outputs.context}\",\n",
    "    },\n",
    "    display_name=\"eval_groundedness_2k_context\",\n",
    ")\n",
    "pf.stream(eval_groundedness_2k_context)\n",
    "\n",
    "print(eval_groundedness_2k_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.answer</th>\n",
       "      <th>inputs.context</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.groundedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main difference between BERT and p...</td>\n",
       "      <td>The main difference between BERT and earlier l...</td>\n",
       "      <td>['BERT: Pre-training of Deep Bidirectional Tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the size of the vocabulary used by BERT?</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>['E (L=12, H=768, A=12, Total Param-\\neters=11...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>论文写作中论文引言有什么注意事项？</td>\n",
       "      <td>在论文写作中，论文引言的要点应该包括：简洁地提出研究问题或研究主题，阐明研究的重要性和必要性...</td>\n",
       "      <td>['须言之成理，论据可靠 ，严格遵循本学科国际通行的学术规范。 内容包括：第 一\\n章引言（...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     inputs.question  \\\n",
       "0  What is the main difference between BERT and p...   \n",
       "1   What is the size of the vocabulary used by BERT?   \n",
       "2                                  论文写作中论文引言有什么注意事项？   \n",
       "\n",
       "                                       inputs.answer  \\\n",
       "0  The main difference between BERT and earlier l...   \n",
       "1                                      I don't know.   \n",
       "2  在论文写作中，论文引言的要点应该包括：简洁地提出研究问题或研究主题，阐明研究的重要性和必要性...   \n",
       "\n",
       "                                      inputs.context  inputs.line_number  \\\n",
       "0  ['BERT: Pre-training of Deep Bidirectional Tra...                   0   \n",
       "1  ['E (L=12, H=768, A=12, Total Param-\\neters=11...                   1   \n",
       "2  ['须言之成理，论据可靠 ，严格遵循本学科国际通行的学术规范。 内容包括：第 一\\n章引言（...                   2   \n",
       "\n",
       "   outputs.groundedness  \n",
       "0                    10  \n",
       "1                     1  \n",
       "2                     3  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_details(eval_groundedness_2k_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'groundedness': 4.666666666666667}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_metrics(eval_groundedness_2k_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'from promptflow import tool' is deprecated and will be removed in the future. Use 'from promptflow.core import tool' instead.\n",
      "WARNING:root:'from promptflow import ToolProvider' is deprecated and will be removed in the future. Use 'from promptflow.core import ToolProvider' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HTML file is generated at 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pf-visualize-detail-vdjdgjdd.html'.\n",
      "Trying to view the result in a web browser...\n",
      "Successfully visualized from the web browser.\n"
     ]
    }
   ],
   "source": [
    "pf.visualize(eval_groundedness_2k_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see a web page like this. It gives you detail about how each row is graded and even the details how the evaluation run executes:\n",
    "![pf-visualize-screenshot](./media/chat-with-pdf/pf-visualize-screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Try a different configuration and evaluate again - experimentation\n",
    "\n",
    "NOTE: since we only use 3 lines of test data in this example, and because of the non-deterministic nature of LLMs, don't be surprised if you see exact same metrics when you run this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-16 12:13:00 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run chat_with_pdf_variant_0_20241116_121300_021422, log path: C:\\Users\\User\\.promptflow\\.runs\\chat_with_pdf_variant_0_20241116_121300_021422\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=chat_with_pdf_variant_0_20241116_121300_021422\n",
      "2024-11-16 12:13:00 +0000   34368 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-11-16 12:13:00 +0000   34368 execution.bulk     INFO     Current system's available memory is 39546.0MB, memory consumption of current process is 301.6015625MB, estimated available worker count is 39546.0/301.6015625 = 131\n",
      "2024-11-16 12:13:00 +0000   34368 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 131}.\n",
      "2024-11-16 12:13:08 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-12)-Process id(10896)-Line number(0) start execution.\n",
      "2024-11-16 12:13:08 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-13)-Process id(11852)-Line number(1) start execution.\n",
      "2024-11-16 12:13:08 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-14)-Process id(14120)-Line number(2) start execution.\n",
      "2024-11-16 12:13:16 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-12)-Process id(10896)-Line number(0) completed.\n",
      "2024-11-16 12:13:17 +0000   34368 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-11-16 12:13:17 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 9.08 seconds. Estimated time for incomplete lines: 18.16 seconds.\n",
      "2024-11-16 12:13:21 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-13)-Process id(11852)-Line number(1) completed.\n",
      "2024-11-16 12:13:22 +0000   34368 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2024-11-16 12:13:22 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 7.05 seconds. Estimated time for incomplete lines: 7.05 seconds.\n",
      "2024-11-16 12:13:24 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-14)-Process id(14120)-Line number(2) completed.\n",
      "2024-11-16 12:13:25 +0000   34368 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-11-16 12:13:25 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 5.72 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-11-16 12:13:25 +0000   34368 execution.bulk     INFO     The thread monitoring the process [14120-SpawnProcess-14] will be terminated.\n",
      "2024-11-16 12:13:25 +0000   34368 execution.bulk     INFO     The thread monitoring the process [11852-SpawnProcess-13] will be terminated.\n",
      "2024-11-16 12:13:25 +0000   34368 execution.bulk     INFO     The thread monitoring the process [10896-SpawnProcess-12] will be terminated.\n",
      "2024-11-16 12:13:25 +0000   14120 execution.bulk     INFO     The process [14120] has received a terminate signal.\n",
      "2024-11-16 12:13:25 +0000   11852 execution.bulk     INFO     The process [11852] has received a terminate signal.\n",
      "2024-11-16 12:13:25 +0000   10896 execution.bulk     INFO     The process [10896] has received a terminate signal.\n",
      "2024-11-16 12:13:26 +0000   34368 execution.bulk     INFO     Process 11852 terminated.\n",
      "2024-11-16 12:13:26 +0000   34368 execution.bulk     INFO     Process 10896 terminated.\n",
      "2024-11-16 12:13:26 +0000   34368 execution.bulk     INFO     Process 14120 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"chat_with_pdf_variant_0_20241116_121300_021422\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-11-16 12:12:59.985283+00:00\"\n",
      "Duration: \"0:00:27.453747\"\n",
      "Output path: \"C:\\Users\\User\\.promptflow\\.runs\\chat_with_pdf_variant_0_20241116_121300_021422\"\n",
      "\n",
      "name: chat_with_pdf_variant_0_20241116_121300_021422\n",
      "created_on: '2024-11-16T12:12:59.985283+00:00'\n",
      "status: Completed\n",
      "display_name: chat_with_pdf_variant_0_20241116_121300_021422\n",
      "description:\n",
      "tags:\n",
      "properties:\n",
      "  flow_path: D:/repos2/rag-data-openai-python-promptflow/notebooks/chat-with-pdf\n",
      "  output_path: C:/Users/User/.promptflow/.runs/chat_with_pdf_variant_0_20241116_121300_021422\n",
      "  column_mapping:\n",
      "    question: ${data.question}\n",
      "    pdf_url: ${data.pdf_url}\n",
      "    chat_history: ${data.chat_history}\n",
      "    config:\n",
      "      EMBEDDING_MODEL_DEPLOYMENT_NAME: text-embedding-ada-002\n",
      "      CHAT_MODEL_DEPLOYMENT_NAME: gpt-4\n",
      "      PROMPT_TOKEN_LIMIT: 2000\n",
      "      MAX_COMPLETION_TOKENS: 256\n",
      "      VERBOSE: true\n",
      "      CHUNK_SIZE: 1024\n",
      "      CHUNK_OVERLAP: 64\n",
      "  system_metrics:\n",
      "    total_tokens: 5221\n",
      "    prompt_tokens: 4958\n",
      "    completion_tokens: 263\n",
      "    duration: 26.806058\n",
      "flow_name: chat-with-pdf\n",
      "data: \n",
      "  D:/repos2/rag-data-openai-python-promptflow/notebooks/chat-with-pdf/data/bert-paper-qna-3-line.jsonl\n",
      "output: \n",
      "  C:/Users/User/.promptflow/.runs/chat_with_pdf_variant_0_20241116_121300_021422/flow_outputs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_3k_context = {\n",
    "    \"EMBEDDING_MODEL_DEPLOYMENT_NAME\": \"text-embedding-ada-002\",\n",
    "    \"CHAT_MODEL_DEPLOYMENT_NAME\": \"gpt-4\",  # change this to the name of your deployment if you're using Azure OpenAI\n",
    "    \"PROMPT_TOKEN_LIMIT\": 3000,\n",
    "    \"MAX_COMPLETION_TOKENS\": 256,\n",
    "    \"VERBOSE\": True,\n",
    "    \"CHUNK_SIZE\": 1024,\n",
    "    \"CHUNK_OVERLAP\": 64,\n",
    "}\n",
    "\n",
    "run_3k_context = pf.run(flow=flow_path, data=data_path, column_mapping=column_mapping)\n",
    "pf.stream(run_3k_context)\n",
    "\n",
    "print(run_3k_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-16 12:14:27 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run eval_groundedness_variant_0_20241116_121427_369215, log path: C:\\Users\\User\\.promptflow\\.runs\\eval_groundedness_variant_0_20241116_121427_369215\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=eval_groundedness_variant_0_20241116_121427_369215\n",
      "2024-11-16 12:14:28 +0000   34368 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-11-16 12:14:28 +0000   34368 execution.bulk     INFO     Current system's available memory is 39504.9296875MB, memory consumption of current process is 303.93359375MB, estimated available worker count is 39504.9296875/303.93359375 = 129\n",
      "2024-11-16 12:14:28 +0000   34368 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 129}.\n",
      "2024-11-16 12:14:36 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-18)-Process id(37144)-Line number(0) start execution.\n",
      "2024-11-16 12:14:36 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-17)-Process id(9032)-Line number(1) start execution.\n",
      "2024-11-16 12:14:36 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-16)-Process id(35148)-Line number(2) start execution.\n",
      "2024-11-16 12:14:37 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-16)-Process id(35148)-Line number(2) completed.\n",
      "2024-11-16 12:14:38 +0000   34368 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2024-11-16 12:14:38 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 2.02 seconds. Estimated time for incomplete lines: 4.04 seconds.\n",
      "2024-11-16 12:14:44 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-17)-Process id(9032)-Line number(1) completed.\n",
      "2024-11-16 12:14:44 +0000   34368 execution.bulk     INFO     Process name(SpawnProcess-18)-Process id(37144)-Line number(0) completed.\n",
      "2024-11-16 12:14:45 +0000   34368 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2024-11-16 12:14:45 +0000   34368 execution.bulk     INFO     Average execution time for completed lines: 3.03 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-11-16 12:14:45 +0000   34368 execution.bulk     INFO     The thread monitoring the process [9032-SpawnProcess-17] will be terminated.\n",
      "2024-11-16 12:14:45 +0000   34368 execution.bulk     INFO     The thread monitoring the process [37144-SpawnProcess-18] will be terminated.\n",
      "2024-11-16 12:14:45 +0000   34368 execution.bulk     INFO     The thread monitoring the process [35148-SpawnProcess-16] will be terminated.\n",
      "2024-11-16 12:14:45 +0000    9032 execution.bulk     INFO     The process [9032] has received a terminate signal.\n",
      "2024-11-16 12:14:45 +0000   37144 execution.bulk     INFO     The process [37144] has received a terminate signal.\n",
      "2024-11-16 12:14:45 +0000   35148 execution.bulk     INFO     The process [35148] has received a terminate signal.\n",
      "2024-11-16 12:14:46 +0000   34368 execution.bulk     INFO     Process 9032 terminated.\n",
      "2024-11-16 12:14:46 +0000   34368 execution.bulk     INFO     Process 35148 terminated.\n",
      "2024-11-16 12:14:46 +0000   34368 execution.bulk     INFO     Process 37144 terminated.\n",
      "2024-11-16 12:14:47 +0000   34368 execution.bulk     INFO     Executing aggregation node...\n",
      "2024-11-16 12:14:47 +0000   34368 execution.bulk     INFO     Finish executing aggregation node.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"eval_groundedness_variant_0_20241116_121427_369215\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-11-16 12:14:27.312387+00:00\"\n",
      "Duration: \"0:00:20.532403\"\n",
      "Output path: \"C:\\Users\\User\\.promptflow\\.runs\\eval_groundedness_variant_0_20241116_121427_369215\"\n",
      "\n",
      "name: eval_groundedness_variant_0_20241116_121427_369215\n",
      "created_on: '2024-11-16T12:14:27.312387+00:00'\n",
      "status: Completed\n",
      "display_name: eval_groundedness_3k_context\n",
      "description:\n",
      "tags:\n",
      "properties:\n",
      "  flow_path: \n",
      "    D:/repos2/rag-data-openai-python-promptflow/notebooks/evaluation/eval-groundedness\n",
      "  output_path: \n",
      "    C:/Users/User/.promptflow/.runs/eval_groundedness_variant_0_20241116_121427_369215\n",
      "  column_mapping:\n",
      "    question: ${run.inputs.question}\n",
      "    answer: ${run.outputs.answer}\n",
      "    context: ${run.outputs.context}\n",
      "  system_metrics:\n",
      "    total_tokens: 4675\n",
      "    prompt_tokens: 4672\n",
      "    completion_tokens: 3\n",
      "    duration: 19.665316\n",
      "flow_name: eval-groundedness\n",
      "data:\n",
      "output: \n",
      "  C:/Users/User/.promptflow/.runs/eval_groundedness_variant_0_20241116_121427_369215/flow_outputs\n",
      "run: chat_with_pdf_variant_0_20241116_121300_021422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_groundedness_3k_context = pf.run(\n",
    "    flow=eval_groundedness_flow_path,\n",
    "    run=run_3k_context,\n",
    "    column_mapping={\n",
    "        \"question\": \"${run.inputs.question}\",\n",
    "        \"answer\": \"${run.outputs.answer}\",\n",
    "        \"context\": \"${run.outputs.context}\",\n",
    "    },\n",
    "    display_name=\"eval_groundedness_3k_context\",\n",
    ")\n",
    "pf.stream(eval_groundedness_3k_context)\n",
    "\n",
    "print(eval_groundedness_3k_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.answer</th>\n",
       "      <th>inputs.context</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>outputs.groundedness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main difference between BERT and p...</td>\n",
       "      <td>BERT se distingue de los modelos anteriores de...</td>\n",
       "      <td>['BERT: Pre-training of Deep Bidirectional Tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the size of the vocabulary used by BERT?</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>['E (L=12, H=768, A=12, Total Param-\\neters=11...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>论文写作中论文引言有什么注意事项？</td>\n",
       "      <td>在撰写论文时，引言部分应严格遵循本学科国际通行的学术规范，内容需要有逻辑性。标题格式应采用“...</td>\n",
       "      <td>['须言之成理，论据可靠 ，严格遵循本学科国际通行的学术规范。 内容包括：第 一\\n章引言（...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     inputs.question  \\\n",
       "0  What is the main difference between BERT and p...   \n",
       "1   What is the size of the vocabulary used by BERT?   \n",
       "2                                  论文写作中论文引言有什么注意事项？   \n",
       "\n",
       "                                       inputs.answer  \\\n",
       "0  BERT se distingue de los modelos anteriores de...   \n",
       "1                                      I don't know.   \n",
       "2  在撰写论文时，引言部分应严格遵循本学科国际通行的学术规范，内容需要有逻辑性。标题格式应采用“...   \n",
       "\n",
       "                                      inputs.context  inputs.line_number  \\\n",
       "0  ['BERT: Pre-training of Deep Bidirectional Tra...                   0   \n",
       "1  ['E (L=12, H=768, A=12, Total Param-\\neters=11...                   1   \n",
       "2  ['须言之成理，论据可靠 ，严格遵循本学科国际通行的学术规范。 内容包括：第 一\\n章引言（...                   2   \n",
       "\n",
       "   outputs.groundedness  \n",
       "0                    10  \n",
       "1                     1  \n",
       "2                     9  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.get_details(eval_groundedness_3k_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HTML file is generated at 'C:\\\\Users\\\\User\\\\AppData\\\\Local\\\\Temp\\\\pf-visualize-detail-pocru2yp.html'.\n",
      "Trying to view the result in a web browser...\n",
      "Successfully visualized from the web browser.\n"
     ]
    }
   ],
   "source": [
    "pf.visualize([eval_groundedness_2k_context, eval_groundedness_3k_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "build_doc": {
   "author": [
    "wangchao1230@github.com",
    "ttthree@github.com"
   ],
   "category": "local",
   "section": "Rag",
   "weight": 10
  },
  "description": "A tutorial of chat-with-pdf flow that allows user ask questions about the content of a PDF file and get answers",
  "kernelspec": {
   "display_name": "Python (azure_ml)",
   "language": "python",
   "name": "azure_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
