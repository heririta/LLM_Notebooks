{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5460247",
   "metadata": {},
   "source": [
    "# PromptFLOW\n",
    "Prompt flow is a suite of development tools designed to streamline the end-to-end development cycle of LLM-based AI applications, from ideation, prototyping, testing, evaluation to production deployment and monitoring. It makes prompt engineering much easier and enables you to build LLM apps with production quality.\n",
    "\n",
    "With prompt flow, you will be able to:\n",
    "\n",
    "- Create flows that link LLMs, prompts, Python code and other tools together in a executable workflow.\n",
    "\n",
    "- Debug and iterate your flows, especially tracing interaction with LLMs with ease.\n",
    "\n",
    "- Evaluate your flows, calculate quality and performance metrics with larger datasets.\n",
    "\n",
    "- Integrate the testing and evaluation into your CI/CD system to ensure quality of your flow.\n",
    "\n",
    "- Deploy your flows to the serving platform you choose or integrate into your app’s code base easily.\n",
    "\n",
    "\n",
    "https://microsoft.github.io/promptflow/index.html\n",
    "\n",
    "\n",
    "## Develop a prompty\n",
    "\n",
    "https://microsoft.github.io/promptflow/how-to-guides/develop-a-prompty/index.html\n",
    "\n",
    "\n",
    "## env variables\n",
    "```\n",
    "AZURE_SUBSCRIPTION_ID= SUBCRIPTION_ID\n",
    "AZURE_RESOURCE_GROUP= RESOURCE GROUP\n",
    "AZUREAI_PROJECT_NAME= AZURE AI STUDIO PROJECT NAME\n",
    "AZURE_OPENAI_CONNECTION_NAME=  Azure OPENai Connection NAME\n",
    "\n",
    "AZURE_OPENAI_ENDPOINT= AZURE OPENAI ENDPOINT URL\n",
    "AZURE_OPENAI_CHAT_DEPLOYMENT= AZURE OPENAI CHAT DEPLOYMENT NAME\n",
    "AZURE_OPENAI_API_VERSION= AZURE OPENAI CHAT DEPLOYMENT VERSION \n",
    "AZURE_OPENAI_API_KEY= AZURE OPENAI KEY\n",
    "\n",
    "RESOURCE_GROUP= RESOURCE GROUP NAME = AZURE_RESOURCE_GROUP\n",
    "SUBSCRIPTION_ID=  AZURE_SUBSCRIPTION_ID\n",
    "AZUREML_WORKSPACE_NAME= AZURE ML NAME \n",
    "TENANTID= TENANT ID SERVICE PRINCIPaL ACCOUNT\n",
    "AZURE_CLIENT_ID=   CLIENT  ID SERVICE PRINCIPaL ACCOUNT\n",
    "AZURE_TENANT_ID=  TENANT ID SERVICE PRINCIPaL ACCOUNT = TENANTID\n",
    "AZURE_CLIENT_SECRET=  SERVICE PRINCIPAL ACCOUNT SECRET\n",
    "\n",
    "```\n",
    "\n",
    "### Requirements\n",
    "\n",
    "```\n",
    "\n",
    "python-dotenv\n",
    "bs4\n",
    "azure-identity\n",
    "azure-search-documents==11.4.0\n",
    "promptflow-tracing==1.11.0\n",
    "promptflow-evals==0.3.0\n",
    "jinja2\n",
    "aiohttp\n",
    "azure-ai-ml==1.16.0\n",
    "promptflow[azure]==1.11.0\n",
    "promptflow-tools==1.4.0\n",
    "promptflow-rag==0.1.0\n",
    "jinja2\n",
    "aiohttp\n",
    "\n",
    "\n",
    "# The following dependencies are required for provisioning\n",
    "\n",
    "# openai SDK\n",
    "openai==1.13.3\n",
    "\n",
    "# azure dependencies\n",
    "azure-core==1.30.1\n",
    "azure-mgmt-authorization==4.0.0\n",
    "azure-mgmt-resource==23.0.1\n",
    "azure-mgmt-search==9.1.0\n",
    "azure-mgmt-cognitiveservices==13.5.0\n",
    "\n",
    "# utilities\n",
    "omegaconf-argparse==1.0.1\n",
    "omegaconf==2.3.0\n",
    "pydantic>=2.6\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982173d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv(\"keys/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5934cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dotenv_values(\"keys/.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8968855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "314e16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from promptflow.tracing import trace\n",
    "from promptflow.core import Prompty\n",
    "# Paths\n",
    "ROOT_DIR = os.getcwd()\n",
    "BASE_DIR = Path(ROOT_DIR).absolute().parent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "357a2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from promptflow.core import Prompty, AzureOpenAIModelConfiguration\n",
    "from promptflow.tracing import trace, start_trace\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f2b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_deployment=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),\n",
    "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=config['AZURE_OPENAI_API_KEY']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b502ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/repos2/rag-data-openai-python-promptflow/notebooks\\\\prompts\\\\chat.prompty'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_prompty = os.path.join(BASE_DIR.cwd().absolute().as_posix(), \"prompts\", \"chat.prompty\")\n",
    "path_to_prompty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8eba6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatPrompty = Prompty.load(\n",
    "        path_to_prompty,\n",
    "        model={\n",
    "            \"configuration\": model_config,\n",
    "            \"parameters\": {\"max_tokens\": 256, \"temperature\": 0.2},\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00d54bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.completion_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute 'llm.usage.prompt_tokens_details' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.completion' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n",
      "WARNING:opentelemetry.attributes:Invalid type NoneType for attribute '__computed__.cumulative_token_count.prompt' value. Expected one of ['bool', 'str', 'bytes', 'int', 'float'] or a sequence of those types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try to prove Celia's statement is false by providing a counterexample. \n",
      "\n",
      "Let's take n = 2, which is a prime number. \n",
      "\n",
      "Substituting n = 2 in the given expression, we get: \n",
      "\n",
      "2n^2 + n + 10 = 2(2)^2 + 2 + 10 = 18 \n",
      "\n",
      "18 is an even number, which contradicts Celia's statement that the expression is always odd when n is a prime number. \n",
      "\n",
      "Therefore, Celia's statement is false.\n"
     ]
    }
   ],
   "source": [
    "from promptflow.tracing import start_trace\n",
    "start_trace()\n",
    "\n",
    "question=\"\"\"Celia states that n n2 10 2 + + is always odd when n is a prime number.\n",
    "Prove that Celia’s statement is false. \n",
    "\"\"\"\n",
    "result = chatPrompty(question=question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361bf40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (azure_ml)",
   "language": "python",
   "name": "azure_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
