{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79ca2477-cba0-4138-af49-186afc59ce26",
   "metadata": {},
   "source": [
    "# BitNet.cpp\n",
    "\n",
    "- https://arxiv.org/abs/2410.16144\n",
    "- https://arxiv.org/pdf/2402.17764\n",
    "- https://www.microsoft.com/en-us/research/publication/bitnet-scaling-1-bit-transformers-for-large-language-models/\n",
    "- https://huggingface.co/HF1BitLLM/Llama3-8B-1.58-100B-tokens\n",
    "- https://github.com/microsoft/BitNet?tab=readme-ov-file\n",
    "\n",
    "  \n",
    "bitnet.cpp is the official inference framework for 1-bit LLMs (e.g., BitNet b1.58). It offers a suite of optimized kernels, that support fast and lossless inference of 1.58-bit models on CPU (with NPU and GPU support coming next).\n",
    "\n",
    "The first release of bitnet.cpp is to support inference on CPUs. bitnet.cpp achieves speedups of 1.37x to 5.07x on ARM CPUs, with larger models experiencing greater performance gains. Additionally, it reduces energy consumption by 55.4% to 70.0%, further boosting overall efficiency. On x86 CPUs, speedups range from 2.37x to 6.17x with energy reductions between 71.9% to 82.2%. Furthermore, bitnet.cpp can run a 100B BitNet b1.58 model on a single CPU, achieving speeds comparable to human reading (5-7 tokens per second), significantly enhancing the potential for running LLMs on local devices. Please refer to the technical report for more details.\n",
    "\n",
    "```\n",
    "@misc{,\r\n",
    "      title={1.58-Bit LLM: A New Era of Extreme Quantization}, \r\n",
    "      author={Mohamed Mekkouri and Marc Sun and Leandro von Werra and Thomas Wolf},\r\n",
    "      year={2024},```\n",
    "}\r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d81bad-5167-48ab-b702-2b491edfd5fb",
   "metadata": {},
   "source": [
    "![image.png](image.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "674945df-a717-4715-93ec-f7dfe8f3b47d",
   "metadata": {},
   "source": [
    "# Quantization Function\n",
    "\n",
    "https://medium.com/@isaakmwangi2018/microsoft-open-sources-1-bit-llms-run-100b-parameter-models-locally-with-bitnet-b1-58-f28aa8c4e702\n",
    "\n",
    "## Scaling Weights\n",
    "\n",
    "![image.png](scaling.png)\n",
    "\n",
    "## Rounding and Clipping\n",
    "\n",
    "![image.png](clip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2adf99-070d-4846-aa87-27dacb790de5",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "![image.png](1.png)\n",
    "\n",
    "### Scaling factor\n",
    "\n",
    "![image.png](2.png)\n",
    "\n",
    "### Scale Matrix\n",
    "\n",
    "![image.png](3.png)\n",
    "\n",
    "### Apply RoundClip Function\n",
    "\n",
    "![image.png](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bde486-56c3-4855-b6d1-d4163f9fb76b",
   "metadata": {},
   "source": [
    "# Install\n",
    "\n",
    "#### Ubuntu /Debian\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt install build-essential libssl-dev -y\n",
    "\n",
    "#### cmake\n",
    "wget https://github.com/Kitware/CMake/releases/download/v3.28.1/cmake-3.28.1.tar.gz\n",
    "tar -zxvf cmake-3.28.1.tar.gz\n",
    "cd cmake-3.28.1\n",
    "sudo ./bootstrap\n",
    "sudo make\n",
    "sudo make install\n",
    "\n",
    "#### clang\n",
    "sudo apt install clang \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db94c98f-1b03-485c-9573-22876760770b",
   "metadata": {},
   "source": [
    "```\n",
    "# Clone the repo\n",
    " git clone git clone −−recursive https://github.com/microsoft/BitNet.git\n",
    " cd BitNet\n",
    "\n",
    "# Create a new conda environment\n",
    "\n",
    " conda create −n bitnet−cpp python=3.9\n",
    " conda activate bitnet−cpp\n",
    " pip install −r requirements.txt\n",
    "\n",
    "#  Install Huggingface hub CLI\n",
    "pip install -U \"huggingface_hub[cli]\"\n",
    "\n",
    "# Download the model from Hugging Face, convert it to quantized gguf format, and build the project\n",
    "# These models were neither trained nor released by Microsoft. We used them to demonstrate the inference capabilities of bitnet.cpp\n",
    "\n",
    "python setup_env.py −−hf−repo HF1BitLLM/Llama3−8B−1.58−100B−tokens −q i2_s\n",
    "\n",
    "# Or you can manually download the model and run it using a local path\n",
    "\n",
    " huggingface−cli download HF1BitLLM/Llama3−8B−1.58−100B−tokens −−local−dir models/Llama3−8B−1.58−100B−tokens\n",
    " python setup_env.py −md models/Llama3−8B−1.58−100B−tokens −q i2_s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e0952-461a-430f-9840-f6e2701b0aad",
   "metadata": {},
   "source": [
    "# INFERENCE\n",
    "\n",
    "##### Ex: 1\n",
    "```\n",
    "python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf -p \\\r\n",
    "\"Daniel went back to the the the garden. Mary travelled to the kitchen. Sandra journeyed to the kitchen. Sandra went to the hallway. John went to the bedroom. \\\r\n",
    "Mary went back to the garden. Where is Mary?\\nAnswer:\" -n 6 -temp\n",
    "```\n",
    "##### Ex: 2\n",
    "```\n",
    "python run_inference.py -m models/Llama3-8B-1.58-100B-tokens/ggml-model-i2_s.gguf -p \\\r\n",
    "\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman.\r\n",
    "Then I went again and bought 5 more apples and I ate 1 apple.\r\n",
    "How many apples did I remain with? Let's think step by step\\nAnswer:\" -n 6 -temp 0.8 -c 40```96\r\n",
    " 0 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94606642-ba15-4002-b501-e9869fe92f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
